{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4dfbce3-ae6c-4397-ae23-4a2ee639ae3d",
   "metadata": {},
   "source": [
    "1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3e2f4-85a1-4b8b-9895-d24cf59ab292",
   "metadata": {},
   "source": [
    "    Scikit-learn can be used to train a machine learning model.\n",
    "    MediaPipe can be used to process input video and extract features (like poses or hand movements).\n",
    "    Matplotlib can be used to visualize the results of the model and the input data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c060f02d-2ac6-40ff-96e9-f009adbc0bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.16.1 opencv-python mediapipe scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bccb0c-608c-4302-bd29-bc0afaac497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9031aa-f631-4cba-af15-538796665d28",
   "metadata": {},
   "source": [
    "2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b36b9-acb1-4db6-9d69-32a683bc883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e433be82-3a53-45cf-88b9-fa3dcd7340fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB) # Color Conversion BGR to RGB\n",
    "    image.flags.writeable = False # image no longer writeable\n",
    "    results = model.process(image) # makes prediction\n",
    "    image.flags.writeable = True # image is now writeable\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR) # Color Conversion RGB to BGR\n",
    "    return image,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e0b9fb-36ee-4321-a70c-4e86a46623dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image,results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS ) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)# Draw hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd61fd1-5a2f-4490-b137-4f05ab4b6368",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7ec68c-6f27-48e3-b9bb-ff39bdf645a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap =cv2.VideoCapture(0)\n",
    "# Set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        # Read feed\n",
    "        ret,frame = cap.read()\n",
    "        \n",
    "        # Makes detection\n",
    "        image, results = mediapipe_detection(frame,holistic)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', frame)\n",
    "        \n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69eead-63f8-46a9-b6e9-082cd726bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_landmarks(frame, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b675d65f-e6bd-442c-8f96-e400ed4d0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f06e158-3193-43de-b96c-2dea7796cac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55ad64-96c4-49dd-8086-415f2f486781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a7f8d-5d5f-4601-93ac-ab909aae4f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
